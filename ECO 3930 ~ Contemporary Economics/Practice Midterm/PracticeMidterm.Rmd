---
title: "Practice Midterm Exam"
subtitle: "ECO 3930 ~ Contemporary Economic Issues"
author: "Gus Lipkin - glipkin6737@floridapoly.edu"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    extra_dependencies: ["amsmath"]
---


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
library(ivreg)
```

# Questions 1-2

   <!-- html_notebook: -->
   <!--  toc: yes -->
   <!--  toc_depth: 2 -->
   <!--  toc_float: yes -->

## 1
> Explain briefly how and why simple models, optimization, equilibrium (particularly supply and demand), and an understanding of property rights together form an important part of the core of microeconomic analysis.

Simple models allow us to take the complex world around us and reduce it to something that is easier to digest and work with. We can then take that model and optimize it so that we can get the best result possible. We can then look at how we can influence the world around us to induce that optimal result. The optimal result is not always the equilibrium result which is the point in supply and demand or in another system that is naturally reached without outside influence. Property rights are deeply intertwined with microeconomics because they define how people perceive ownership of goods. Wikipedia notes that property rights have three attributes and that people have "the right to use the good," "the right to earn income from the good," and "the right to transfer the good to others, alter it, abandon it, or destroy it (the right to ownership cessation)."

## 2
> Suppose riding coaster A provides exactly the same total satisfaction as riding coaster B, and that each seats 50 riders per cycle, but that coaster A takes only 2/3 as much time as coaster B to complete a cycle. Use the idea of equilibrium to estimate the length of the line for B relative to the line for A. Explain your approach.

\begin{gather*}
50a=\frac{2}{3}50b\\
50a=33.33b
\end{gather*}

Let's say that there are $50$ people in line for only ride B, the line will move fully in time $T$. If there are $50$ people in line for ride A, the line will take time $\frac{2}{3}T$. People will want the lines to take as little time as possible, and thus gravitate towards the line that takes the least amount of time. If there are 50 people or less, this is relatively simple. They will choose ride A. However, if there are more than 50 people in line for ride A, then they would want to know the proportion. From the math above, we learn that the ride wait times will be equal if the number of people in line B is two thirds that of line A, with the equation being $a=\frac{2}{3}b$ where $a$ and $b$ are the number of people in each line. If the $b$ side of the equation is larger, then people will shift towards ride A. If $a$ is larger, people will go to ride B.

# Questions 3-6
> **One-hundred people fish in two lakes, East Lake and West Lake, on any given day. Each may choose to fish in either lake. Let $n$ be the number that fish in East Lake and $100-n$ be the number that fish in West Lake. West Lake is very large and relatively plain with dispersed fish populations. The value someone receives fishing in W is $20$. East Lake is spectacular with concentrated fish populations. The value someone gets from fishing in East Lake is $50-0.5n$.**

## 3
> If people individually choose which lake to fish in, how many will fish in East Lake and how many in West Lake, and what is the total value to everyone fishing? (Total value is just the number fishing East multiplied by the individual value of dishing in East, plus the number fishing in West multiplied by the individual value of fishing in West.)

\begin{gather*}
20=50-.5n\\
-30=-.5n\\
n=60
\end{gather*}

The number of people fishing in East Lake is $60$ which leaves $40$ people in West Lake. 

\begin{gather*}
20*40=800\\
60*(50-.5(60))=1200\\
800+1200=2000
\end{gather*}

The total value of everyone fishing in the lake is $2000$ where the people in West Lake have a combined total of $800$ and East Lake is $1200$.

## 4
> If a benevolent dictator could limit the number fishing in East, how many people fishing in East and how many in West would maximize total value, and what would total value be?

```{r}
fishW <- 0
fishE <- 0
maxP <- 0
wMax <- 0
eMax <- 0
max <- 0

for(i in 0:100) {
  fishE <- i
  fishW <- 100 - fishE
  max <- (20*fishW)+(fishE*(50-(.5*fishE)))
  if(max > maxP) {
    eMax <- fishE
    wMax <- fishW
    maxP <- max
  }
}
paste(maxP, eMax, wMax)
```

The maximum profit is $2450$. This happens when the number of people fishing in East Lake is $30$ and the number of people fishing in West Lake is $70$.

## 5
> Relate the difference between outcomes in #3 and #4 to property rights and the tragedy of the commons.

The tragedy of the commons is that every person is seeking to maximize their own profit, rather than the profit of the community, or the commons. In question three, people seek to maximize their own profit, and thus the total profit is only $2000$. In question four, the benevolent Dr Dewey decides to maximize collective profit and the total profit becomes $2450$.

## 6
> Suppose the government can charge a fee to fish in East Lake. How large would the fee need to be to induce the optimum number of people to fish there? How much revenue would the government collect?

\begin{gather*}
V=v(x)\\
v(w)=v(e)-f\\
20=50-.5(n)-f\\
20=50-.5n-f\\
-30=-.5n-f\\
30=.5n+f\\
0=.5n+f-30
\end{gather*}

```{r}
n <- 30

for(f in 0:1000) {
  if (0 == (.5 * n) + f - 30) {
    break
  }
}

print(paste0("n: ", n, ", f: ", f))
print(paste0("Total profit: ", 20*(100-n) + (n*(50-(.5*n))-(n*f))))
```

I could be totally wrong, but I don't want to think I am. From above, we have the equation $v(w)=v(e)-f$ where $v(w)$ and $v(e)$ are the profit functions for West and East Lakes respectively, and $f$ is the fee applied to fishing in East Lake. Written out, that is $20(100-n)=n(50-.5n)-fn$ where $n$ is the number of people fishing in East Lake. Since we know that the optimal result is achieved at $n=30$, we can constrain the loop as such and iterate over a value for the fee, $f$, and we get $f=15$ which is a total government revenue of $n*f=30*15=450$.

# Questions 7-10

## 7 
> Discuss the limitations of randomized control trials covered in the article by Deaton and Cartwright.

The single biggest issue with randomized controlled trials is that you cannot make predictions for which you have no data. So even though your RCT may cover an extremely wide swathe of the population, there are people outside that group who you cannot make predictions for. As you increase the size of the study, the costs and complexity increase and so researchers must find an appropriate balance between the size and cost.

## 8
> Discuss what gave rise to the credibility revolution.

Everyone was publishing papers about whatever hot topic they could find, but if more than one person studied the same thing, they were likely to make different assumptions about the topic, and thus influence the outcome of the study. This gave rise to the credibility revolution where researchers looked to hold each other accountable for their research assumptions and to design better studies.

## 9 
> List and discuss several factors that combined to increase the credibility of empirical economics over the course of the credibility revolution.

- Sensitivity Analysis: Adding sensitivity analysis allows research authors to test their assumptions and see how much their results would change if everything was just a little bit different.
- Randomized Controlled Trials: As discussed above, RCTs can be a powerful tool to conduct studies which might otherwise not be possible. While not common, they have been used in cases such as Andrew Yang's Universal Basic Income pilot program.
- Better and More Data: More and better data seems like a given. As the quality and quantity increase, your studies will have better results that are more representative of the world at large.
- Better Research Design: Again, this seems like a given. As the quality of the research design improves, then the quality of the results should improve as well.

## 10 
> Discuss a potential downside of the credibility revolution.

One criticism, called `External Validity` is that study based research is too specific and not suited for the world at large. In essence, the need for Better Research Design has created a problem where the studies are so narrow in scope in order to control for outside influences, the study may not be useful in general.

# All Remaining Questions
> **You are interested in whether a tutoring program improves performance. You have a measure of initial ability, $S_1$. You know who signed up for and received voluntary tutoring, $T$. You later observe a second measure of performance, $S_2$. You are worried unobservables, $U$, influence the initial score, the probability of signing up for tutoring for a given initial score, and the second score.**

## 11 
> Give some examples of unobservables relevant to this case.

- Some students may or may not drink coffee before the exam
- Some students may have overbearing parents that insist on their little munchkin being given tutoring, regardless of $S_1$
- Some students may have had a panic attack during the exam due to the lack of a clock in the exam room. Ultimately, a clock was installed before $S_2$

## 12 
> Draw the DAG.

```{r, echo=FALSE}
library(webshot)
DiagrammeR::mermaid("
graph LR
A(U) -->|Unobserved|B[S1]
    B --> D
    A -->|Unobserved|C{T}
    A -->|Unobserved|D[S2]
    B --> C
    C --> D
")
```


## 13 
> What is a backdoor path? Why do you have to block them? How do you block them? Based on the DAG, what backdoor path or paths, if any, can’t be blocked with the data you have?

A backdoor is an alternative method to reach the same node in a DAG. Backdoor paths are not causal like direct paths are, because they are indirect. Backdoors have the ability to create spurious correlations and red herrings about the causal effects on the dependent variable. If you leave a backdoor open, it leaves room for bias to enter the model and so you must block them in order to be able to properly make an argument for a causal effect.

In this DAG, our confounding variable is $S_1$ because it leads both directly to $S_2$ and to $S_2$ through $T$.

# Questions 14-17
> **Suppose there are only four students and the data is as shown in the table ~~to the right~~ below.**
> 
>  Subject | $Y^1$ | $Y^0$ | $T$  
>  :-----: | :---: | :---: | :--:
>  1       | 84    | 76    | 1    
>  2       | 76    | 82    | 0    
>  3       | 74    | 64    | 1    
>  4       | 80    | 78    | 0    

## 14
> What are the observed sample difference, ATT, ATU, ATE, and selection bias?

Create the data table
```{r}
dt <- data.table(
  subject = c(1, 2, 3, 4),
  y1 = c(84, 76, 74, 80),
  y0 = c(76, 82, 64, 78),
  t = c(1, 0, 1, 0)
)
```

Calculate sample difference
```{r}
dt[, diff := y1 - y0][,.(subject, diff)]
```

Calculate Selection Bias
```{r}
mean(dt[t == 1, y0]) - mean(dt[t == 0, y0])
```

Calculate ATE, ATT, and ATU
```{r}
dt[, TE := y1 - y0][t == TRUE, TT := TE][t == FALSE, TU := TE]
data.table(
  ATE = c(mean(dt$TE)),
  ATT = c(mean(dt$TT, na.rm = T)),
  ATU = c(mean(dt$TU, na.rm = T))
)
```

## 15
> Why, in reality, can’t you actually calculate ATT, ATU, ATE, or selection bias? That is, what is the fundamental problem with causal inference?

You can either do something or you can't do something. In this case, you can't calculate ATT, ATU, ATE, or selection bias because you cannot know both outcomes at once. Thus, the fundamental problem with causal inference is that you cannot calculate the real and counterfactual outcomes. 

## 16
> Can big data or AI, on their own, possibly resolve this fundamental problem of causal inference? Why or why not?

Big Data and AI are great buzzwords for people that don't know any better. Although big data and AI may be able to give us better and better predictions about what would happen if we could both do and not do something, the laws of linear time are not affected until we have an AI so good that it builds a time machine (which may or may not be possible, but it certainly hasn't happened yet).

## 17
> How does random assignment help break selection bias?

I feel like this should be kinda obvious from the words, but okay. You can have bias in your selection. If your assignment is totally random, then there is no selection bias. In the Marvel Cinematic Universe, this was Thanos' whole big thing. He would come in and erase half of all life on any given planet with a completely random mechanism. He was very proud of the fact, that he had no selection bias and he thought that this made the process very fair.

# Questions 18-21
> **Assume treatment was randomly assigned.**

## 18
> What exactly is Fisher’s sharp null in this context? Why is Fisher’s sharp null particularly useful for randomization inference?

Fisher wants us to assume that the treatment effect is zero. With this, we can calculate the distribution of sample differences by calculating every from every data combination available in the sample. This is helpful in randomization inference because it allows us to minimize uncertainty from unobserved potential outcomes. Basically, if we're checking against ourselves, then we should be able to detect any bias in the results.

## 19
> Show there are 6 possible ways to assign treatment to 2 of 4 students.

```{r}
combn(4, 2)
```

I admit that I cheated a little with the code above, but it works. There are six combinations of choosing two students from a group of four. The general formula is $C(n,r)=\frac{n!}{r!(n-r!)}$. We can plug in and get: 

\begin{gather*}
C(4,2)=\frac{4!}{2!(4-2)!}\\
C(4,2)=\frac{24}{2!(2)!}\\
C(4,2)=\frac{24}{4}\\
C(4,2)=6
\end{gather*}

## 20
> With the small sample you can construct all possible assignments manually. What are the six sample differences under Fisher’s sharp null?

```{r}
for(c in 1:ncol(combn(4, 2)))
  print(paste0(dt[combn(4, 2)[1,c], y1] - dt[combn(4, 2)[2,c], y0], " or ", 
               dt[combn(4, 2)[2,c], y0] - dt[combn(4, 2)[1,c], y1]))
```

## 21
> Discuss how strong the evidence is for an actual effect based on this (small) sample. Basically, what is the relevant p-value and what do you make of it?

```{r}
t.test(dt[t == 0, y1])
```

The null hypothesis, $H_0$, is that there is no causal effect, while the alternative, $H_1$, is that there is a causal effect. With a p-value of $.01632$, which is less than the significance level of $.05$, we reject the null that there is no causal effect and conclude that there is a significant difference. However, because the sample size is so small, it is difficult to rule one way or another. We could decrease the acceptable p-value, but the next common level is at $.01$, which this test fails.

# Questions 22-25
> **Suppose you have a sample of 400 with 200 randomly treated.**

## 22
> Show the expression for the number of ways to randomly choose 200 from 400. Don’t bother trying to calculate how many that is. Why not?

$$
C(400,200)=\frac{400!}{200!(400-200)!}
$$

For one, *400!==64034522846623895262347970319503005850702583026002959458684445942802397169186831436278478647463264676294350575035856810848298162883517435228961988646802997937341654150838162426461942352307046244325015114448670890662773914918117331955996440709549671345290477020322434911210797593280795101545372667251627877890009349763765710326350331533965349868386831339352024373788157786791506311858702618270169819740062983025308591298346162272304558339520759611505302236086810433297255194852674432232438669948422404232599805551610635942376961399231917134063858996537970147827206606320217379472010321356624613809077942304597360699567595836096158715129913822286578579549361617654480453222007825818400848436415591229454275384803558374518022675900061399560145595206127211192918105032491008000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000*, and *200!==788657867364790503552363213932185062295135977687173263294742533244359449963403342920304284011984623904177212138919638830257642790242637105061926624952829931113462857270763317237396988943922445621451664240254033291864131227428294853277524242407573903240321257405579568660226031904170324062351700858796178922222789623703897374720000000000000000000000000000000000000000000000000* and I'm a pretty smart guy, but I'm not smart enough for this shit. The end result is a really big number that is so big it has lost all meaning.

## 23
> Practically, how can we approximate the Fisher exact test p-value with large samples?

As the dataset gets larger and larger, the Fisher exact test p-value becomes approximately equal to the chi-squared distribution and thus it is acceptable to use a chi-square test instead.

## 24
> Now suppose treatment is not randomly assigned. Instead, everyone who scored under a 60 got extra tutoring and no one else did. Explain how a regression discontinuity design can get at the causal effect of tutoring. Draw a picture to illustrate the LATE.

```{r, echo=FALSE, warning=FALSE}
q24 <- data.table(
  xx = c(0, 60, 60, 100),
  yFuzzy = c(0, 25, 75, 100)
)

ggplot() +
  geom_point(data = q24, aes(x = xx, y = yFuzzy)) +
  stat_smooth(data = q24[1:2], aes(x = xx, y = yFuzzy),
              method = "lm", formula = "y~x", color = "purple", fill = "purple") +
  stat_smooth(data = q24[3:4], aes(x = xx, y = yFuzzy),
              method = "lm", formula = "y~x", color = "orange", fill = "orange") +
  geom_line(data = data.table(xx = c(60, 60), yy = c(50, 75)), aes(x = xx , y = yy), 
            arrow = arrow(length=unit(0.50,"cm")), size = 1) +
  geom_line(data = data.table(xx = c(60, 60), yy = c(50, 25)), aes(x = xx , y = yy), 
            arrow = arrow(length=unit(0.50,"cm")), size = 1) +
  geom_label(aes(x = 60, y = 50, label = "LATE"), size = 5) +
  labs(title = "Fuzzy RDD",
       subtitle = "(Just pretend these are curves like we always do for supply and demand)") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```

A regression discontinuity design would allow us to split the results of people who received tutoring and people who did not. This does two things. One, the data is broken down into relevant "buckets" so that one group cannot affect the results for the other group as they would if both groups were lumped together. And two, so that we can compare across groups to see if there was any true effect.

## 25
> What is a LATE? Why can we only estimate the LATE, not the ATE? Why do you often need a lot of data to make an RDD work?

The local average treatment effect, or LATE, is the size of the gap between regression lines at the cut point in a regression discontinuity graph. We can only estimate the LATE because, like I said in question 15, we cannot know the outcome of both testing and not-testing everyone. As for why we often need a lot of data for an RDD, that's because the more data there is, the more data that is likely to be at the cut point. We also need more data because we will be splitting the dataset into groups and need a good amount of data on each side of the cut-point.

# Questions 26-27
> **Now suppose everyone with an initial score under 60 was assigned to tutoring, but some did not take it, and that a couple of students who scored a bit over 60 were able to get tutoring anyway.**

## 26
> How would you go about employing regression discontinuity to get at the causal effect in this case? What do you use as an instrument for treatment? Why do you need an instrument? What is an instrumental variable, anyway?

Here we would set our discontinuity cut point at $60$, the cutoff for tutoring. From that, we'd be able to see if the people who got tutoring (a score $\le60$) had a significantly different result than those who scored above a $60$ and did not receive tutoring. An instrumental variable is the variable that causes the treatment, but does not directly cause the outcome. In this case, the instrument is the tutoring cutoff point score. We need an instrument to be able to determine a cutoff point in the results. Without the instrument, any cutoff point we determined would be speculation at best because there would be no real reason to separate the data before and after that point.

## 27
> Draw a figure clearly illustrating the difference between sharp and fuzzy RDD. Hint: This is not about the outcome measure.

```{r, echo=FALSE, warning=FALSE}
q27 <- data.table(
  xx = c(0, 5, 5, 10),
  ySharp = c(0, 0, 10, 10),
  yFuzzy = c(0, 2.5, 7.5, 10)
)

sharp <- ggplot() +
  geom_point(data = q27, aes(x = xx, y = ySharp)) +
  stat_smooth(data = q27[1:2], aes(x = xx, y = ySharp),
              method = "lm", formula = "y~x", color = "purple", fill = "purple") +
  stat_smooth(data = q27[3:4], aes(x = xx, y = ySharp),
              method = "lm", formula = "y~x", color = "orange", fill = "orange") +
  geom_line(data = data.table(xx = c(5, 5), yy = c(0, 10)), aes(x = xx , y = yy), linetype = "dashed") +
  labs(title = "Sharp RDD", subtitle = "\n") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank())

fuzzy <- ggplot() +
  geom_point(data = q27, aes(x = xx, y = yFuzzy)) +
  stat_smooth(data = q27[1:2], aes(x = xx, y = yFuzzy),
              method = "lm", formula = "y~x", color = "purple", fill = "purple") +
  stat_smooth(data = q27[3:4], aes(x = xx, y = yFuzzy),
              method = "lm", formula = "y~x", color = "orange", fill = "orange") +
  geom_line(data = data.table(xx = c(5, 5), yy = c(2.5, 7.5)), aes(x = xx , y = yy), linetype = "dashed") +
  labs(title = "Fuzzy RDD", subtitle = "(Just pretend these are curves like we\nalways do for supply and demand)") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank())

gridExtra::grid.arrange(sharp, fuzzy, ncol = 2)
```

# Questions 28-32
> **Data on 400 (hypothetical) observations for these questions is in the accompanying file “tutoring.csv”.**

```{r}
tut <- fread("tutoring.csv")
head(tut)
```

## 28
> Calculate the simple sample difference.

```{r}
tut$diff <- tut$s2 - tut$s1
mean(tut$diff)
```


## 29
> Bin the data in 5-point increments, letting each bin be represented by its midpoint. For each bin, calculate the percent that were treated. Plot this against the bin midpoint. Plot a vertical line at the cut point. Interpret the figure.

```{r}
cutPoint <- 60
bins <- seq(2.5,100,5)

q29 <- tut[, c("s1Bin", 
               "s2Bin") := .(cut(tut$s1, breaks = seq(0,100,5), labels = bins), 
                             cut(tut$s2, breaks = seq(0,100,5), labels = bins))][
            order(s1Bin, t), .N, by = .(s1Bin, t)][
            , percTreatedBin := round(N / sum(N) * 100, 2), by = s1Bin][
            t == 1, !c("t")]

q29$s1Bin <- as.numeric(levels(q29$s1Bin)[q29$s1Bin])
head(q29)
```

```{r}
q29 %>%
  ggplot() +
  geom_point(aes(x = s1Bin, y = percTreatedBin)) +
  geom_vline(xintercept = cutPoint, color = "black") +
  scale_x_continuous(breaks = bins) +
  scale_y_binned(n.breaks = 10, limits = c(0,100))
```

The chart shows us the percent of people in each bin that were treated with tutoring. Before the cutoff point, the people were much more likely to receive tutoring, with an average of about 75% receiving tutoring across bins. After the cutoff point, people were much less likely to receive tutoring, with an average of about 15% across bins.

## 30
> For each bin from the previous question, calculate the average score on the second exam. Plot this against the bin midpoint. Plot a vertical line at the cut point. Interpret the figure. 

```{r}
q30 <- tut[, .(s2Mean = round(mean(s2)), 2), by = "s1Bin"][order(s1Bin), -c("V2")]
q30$s1Bin <- as.numeric(levels(q30$s1Bin)[(q30$s1Bin)])

q30 %>%
  ggplot() +
  geom_point(aes(x = s1Bin, y = s2Mean)) +
  geom_vline(xintercept = cutPoint) +
  scale_x_continuous(breaks = bins) +
  scale_y_binned(n.breaks = 10, limits = c(50, 85))
```

The mean score by bucket of `s2Mean` generally increases as the value of `s1Bin` increases. However, `s2Mean` for those who received tutoring is slightly higher just below the cutoff point than it is for those just after the cutoff point. This could be an indication that those close to the point just needed a little bit of extra help.

## 31
> Calculate the local effect of intention to treat. You will need to do a regression to accomplish this.

```{r, warning=FALSE}
ggplot() +
  stat_smooth(data = q30[s1Bin <= cutPoint], aes(x = s1Bin, y = s2Mean), 
              method = "lm", formula = "y~x", color = "orange4", fill = "orange", fullrange = TRUE) +
  stat_smooth(data = q30[s1Bin > cutPoint,], aes(x = s1Bin, y = s2Mean),
              method = "lm", formula = "y~x", color = "purple4", fill = "purple", fullrange = TRUE) +
  geom_point(data = q30, aes(x = s1Bin, y = s2Mean), size = 2) +
  geom_vline(xintercept = cutPoint) +
  scale_x_continuous(breaks = bins) +
  scale_y_binned(n.breaks = 10, limits = c(50, 85))

below <- lm(formula = s2Mean ~ s1Bin, data = q30[s1Bin <= cutPoint])
above <- lm(formula = s2Mean ~ s1Bin, data = q30[s1Bin > cutPoint])

below
above
```


## 32
> Calculate an estimate of the LATE. You will need to use an instrumental variables estimator.

```{r}
b <- as.numeric(below$coefficients[1]) + as.numeric(below$coefficients[2] * 60)
a <- as.numeric(above$coefficients[1]) + as.numeric(above$coefficients[2] * 60)
b - a
```

```{r}
ivreg(s2 ~ s1 | t, data = tut)
lmtest::coeftest(ivreg(s2 ~ s1 | t, data = tut), type = "HC1")
```

I've done two different things and I'm not sure which is right, or even what to do with the information on the second. The first attempt just found the value of the regression line at `x==60` and took the difference between the two. This makes sense to me since the LATE is the difference between the two regression lines at the cutpoint.

