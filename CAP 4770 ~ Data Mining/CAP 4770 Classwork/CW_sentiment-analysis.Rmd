---
title: "Intro to Sentiment Analysis"
output: html_notebook
---

Reference book: 
<https://www.tidytextmining.com/>

Sentiment analysis can be thought of as the exercise of taking a sentence,
paragraph, document, or any piece of natural language, and determining
whether that text's emotional tone is positive, negative or neutral. One way to
analyze the sentiment of a text is to consider the text as a combination of its
individual words and the sentiment content of the whole text as the sum of the
sentiment content of the individual words.

When human readers approach a text, we use our understanding of the _emotional intent_ of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. 


```{r, echo = FALSE, out.width="20%"}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tidy_text_mining.png")
```




```{r, message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(textdata)
```

The `tidytext` package contains `sentiments` dataset. The three general-purpose lexicons are:

- `AFINN` from [Finn Arup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),

- `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and

- `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).


The above different sentiment lexicons, based on unigrams (i.e. single words) are derived from a single English word and are assigned different scores of positive/negative sentiments. Each lexicon scores a different way from the others. 

- `AFINN` scores a word with a number, which may range from -5 to +5. 

- `bing` scores a word as either positive or negative. 

- `nrc` categorizes a word under sentiment type categories such as positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.


All of this information is tabulated in the sentiments dataset, and `tidytext` provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon.

```{r, eval=FALSE}
# sample of sentiments
sample_n( get_sentiments("afinn") , 5)
```

```{r, eval=FALSE}
# sample of sentiments
sample_n( get_sentiments("nrc") , 5)
```

```{r, eval=FALSE}
# sample of sentiments
sample_n( get_sentiments("bing"), 5)
```


There are also some domain-specific sentiment **lexicons** available, constructed to be used with text from a specific content area. 

Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in "no good" or "not true"; a lexicon-based method like this is based on unigrams only. 

One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.



# Songs Lyrics Example


Read data

```{r, message=FALSE}
# data from https://raw.githubusercontent.com/reisanar/datasets/master/BB_top100_2015.csv
lyrics_2015 <- read_csv("BB_top100_2015.csv")
lyrics_2015 <- select(lyrics_2015, -Year, -Source)
```

Take a look at the data you loaded:

```{r}
head(lyrics_2015)
```

Let us focus on artists that have more than 1 song in the Top 100 

```{r}
# print artist that appear more than once in the list
lyrics_2015[duplicated(lyrics_2015$Artist),]
```

Goal: study the sentiment of the top 10 songs lyrics


```{r}
lyrics_2015 %>% 
  filter(Rank %in% 1:10)
```


First, let us convert the text to the tidy format using `unnest_tokens()`. Notice that we choose the name `word` for the output column from `unnest_tokens()`. This is a convenient choice because the sentiment lexicons and stop word datasets have columns named `word`; performing inner joins and anti-joins is thus easier.


```{r}
lyrics_2015 %>% 
  filter(Rank %in% 1:10) %>%
  unnest_tokens(word, Lyrics)
```

Notice that we did not remove any stop-words!

```{r}
lyrics_2015 %>%
  filter(Rank %in% 1:10) %>%
  unnest_tokens(word, Lyrics, token = "words") %>% filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
```


We could explore this dataset by first, checking the frequency of words in the top 10 songs lyrics.

First let us store our tidy text data set in a new object called `bb_top_10`

```{r}
# BB top 10 songs lyics in 2015 (no stop-words)
bb_top_10 <- lyrics_2015 %>% 
  filter(Rank %in% 1:10) %>% unnest_tokens(word, Lyrics, token = "words") %>% 
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
```

Check the most used words:

```{r}
bb_top_10 %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses)) %>%
  head(10)
```

With the information above, we can graphically show a sample of frequent words

```{r}
 bb_top_10 %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses)) %>%
  slice(1:15) %>%
  ggplot() +
  geom_bar(aes(x = word, y = uses), stat = "identity") +
  coord_flip() +
  theme_minimal()
```




## Sentiment Analysis 

Much as removing stop words is an anti-join operation, performing sentiment analysis is an inner join operation.

```{r}
bb_top_10 %>%
  inner_join(get_sentiments("bing")) %>%
  count(Song, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

And a visualization of it can tell us about the "sentiment" of each song as a whole:

```{r}
bb_top_10 %>%
  inner_join(get_sentiments("bing")) %>%
  count(Song, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot() +
  geom_bar(aes(x = reorder(Song, sentiment), y = sentiment), stat = "identity") +
  coord_flip() +
  labs(x = "", title = "Sentiment Analysis of Songs", subtitle = "Top 10 Billboard songs in 2015") +
  theme_minimal()
```


What if we want to find out how the sentiment of the song varies as the song progresses? Let us use now the `afinn` lexicon

```{r}
bb_top_10 %>%
  inner_join(get_sentiments("afinn"))
```


```{r}
# add an index column to keep track of the words used 
bb_top_10 %>%
  group_by(Song) %>%
  mutate(index = row_number() ) %>%
  inner_join(get_sentiments("afinn"))
```


Notice that several songs in this top 10 list, contain many words that are not part of the lexicon, and therefore were not scored:

```{r}
bb_top_10 %>%
  group_by(Song) %>%
  mutate(index = row_number() ) %>%
  inner_join(get_sentiments("afinn")) %>%
  ggplot() +
  geom_col(aes(x = index, y = value)) +
  facet_wrap(~Song, scales = "free", ncol = 2) +
  labs(title = "Sentiment Analysis of Songs using AFINN lexicon",
       subtitle = "Top 10 Billboard songs in 2015",
       x = "") +
  theme_minimal()
```


# Focus on one artist



Goal: perform sentiment analysis using the `nrc` lexicon for songs by Taylor Swift

```{r}
lyrics_2015 %>%
  filter(Artist == "taylor swift")
```


First, let us convert the text to the tidy format using `unnest_tokens()`. Notice that we choose the name `word` for the output column from `unnest_tokens()`. This is a convenient choice because the sentiment lexicons and stop word datasets have columns named `word`; performing inner joins and anti-joins is thus easier.


```{r}
lyrics_2015 %>%
  filter(Artist == "taylor swift") %>%
  unnest_tokens(word, Lyrics)
```

Notice that we did not remove any stop-words!


We could explore this dataset by first, checking the frequency of words in the top 10 songs lyrics.

First let us store our tidy text data set in a new object

```{r}
# taylor swift songs (no stop-words)
swift <- lyrics_2015 %>%
  filter(Artist == "taylor swift") %>%
  unnest_tokens(word, Lyrics) %>%
  anti_join(stop_words)
```

Check the most used words:

```{r}
swift %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses))
```

With the information above, we can graphically show a sample of frequent words

```{r}
swift %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses)) %>%
  slice(1:15) %>%
  ggplot() +
  geom_bar(
    aes(x = reorder(word, uses), y = uses), stat = "identity") +
  coord_flip() +
  labs(y = "frequency of use", x = "") +
  theme_minimal()
```




## Sentiment Analysis 

Using the `nrc` lexicon

```{r}
swift %>%
  inner_join(get_sentiments("nrc")) %>%
  group_by(sentiment) %>%
  count() %>%
  ggplot() +
  geom_bar(
    aes(x = reorder(sentiment, n), y = n), stat = "identity") +
  labs(title = "Sentiment analysis using nrc lexicon", subtitle = "Lyrics of Songs by Taylor Swift on 2015 Billboard Top 100",  x = "", y = "word-count") +
  theme_minimal()
```

Show words associated to some of the sentiments:

```{r}
swift_words <- swift %>%
  inner_join(get_sentiments("nrc")) %>%
  group_by(sentiment, word) %>%
  count(mycount = n()) %>%
  distinct() %>% filter(sentiment %in% c("negative", "anger",
                                         "positive", "trust"))
# plot
ggplot(data = swift_words, aes(label = word)) +
  ggrepel::geom_label_repel(
    aes(
      x = word,
      y = rnorm(nrow(swift_words)),
      label = word),
    direction = "both",
    box.padding = 0.04,
    segment.color = "transparent",
    size = 3) +
  facet_wrap( ~ sentiment, ncol = 2) +
  labs(x = "", y = "") +
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    panel.background = element_blank())
```
